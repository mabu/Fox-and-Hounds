\documentclass[a4paper]{article}
\usepackage{clrscode}
\usepackage{graphicx}

\title{Q-learning for Fox and Hounds Board Game}
\author{Javad Bakhshi \and Martynas Budriunas}
\date{2012-05-29}

\begin{document}

\maketitle

\begin{abstract}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse
pellentesque enim eget nunc dapibus id imperdiet sapien dictum. Fusce nulla
sem, fringilla in fermentum ac, pretium eget purus. Sed vel dui diam, ac mattis
nisl. Suspendisse risus metus, ullamcorper at ultrices ac, mollis quis urna.
Nunc libero felis, consectetur eget elementum id, malesuada eu purus. Cras
commodo tincidunt purus, sit amet imperdiet ipsum vestibulum ut. Aenean nec
sapien eget lectus feugiat interdum. In malesuada lorem sed ligula auctor a
dignissim sem auctor. Sed et nisi sit amet turpis tincidunt pulvinar ut non
ante. Aliquam vitae urna dui. Aenean in nisl. 
\end{abstract}

\section{Introduction}
Fox and Hounds is a simple two-player board game. Traditionally it is played on
a chess board. One player controls the fox and the other controls four hounds.
Players have different goals. The fox has to reach the other side of the board,
and the hounds have to surround the fox so that it cannot move.

Initially the hounds are placed on all black squares in one row on the edge of
the board. The fox can choose a starting position on any of black squares in the
row on the opposite edge of the board. The pieces can only move to unoccupied
adjacent black squares. Additionally, the hounds can only move forward. The
players take alternative turns with the fox moving first. On each turn only one
of the hounds can be moved. The player who cannot make a move loses.

Fox and Hounds is an easy game aimed at young children. A human player can
easily find an optimal strategy for the hounds which guarantee victory (such a
strategy is described in \cite{berlekamp82}). In this project we try to
investigate how good a computer can be in finding that solution. To do that we
have implemented learning systems for both players based on Q-learning.

\section{Q-learning}
Q-learning is a reinforcement learning technique first presented in
\cite{watkins89}. Like in other reinforcement learning techniques, the agent is
presented with a state and can do one of several actions and the environment
responds by giving a reward. Using this technique each state-action pair $(s,
a)$ is assigned a Q-value $Q(s, a)$ which represents how much the agent favours
taking the action $a$ from a state $s$. In $\epsilon$-greedy Q-learning an agent
chooses a random action with probability $\epsilon$, and with probability $1 -
\epsilon$ it acts greedily -- performs an action with highest Q-value. After
going to a state $s\prime$ and receiving a reward $r$, a Q-value is updated
using the following rule:
\[
    Q(s, a) \gets Q(s, a) + \eta [r + \gamma \max_{a\prime} Q(s\prime, a\prime)
    - Q(s, a)]
\]

Here $\eta$ is a learning rate and $\gamma$ is a discount factor. This way the
agent is trained to seek higher sum of future rewards.

Q-learning was applied to various board games. A very successful example is
TD\mbox{-}Gammon \cite{gerald95} -- a backgammon player which uses
temporal-difference learning, a generalization of Q-learning.

\section{Implementation}
In this game the players have different roles, so a separate learning system is
needed for both of them.

As shown in \cite{berlekamp82}, the hounds have a winning strategy, so to speed
up the learning we made the game slightly easier for the fox -- it wins as soon
as it reaches the same row as the last hound.

\subsection{States}
Many games are troublesome to be solved by Q-learning because of a large number
of states. This game, however, has a number of states small enough to be stored
in a table. The number of states can be calculated as follows. There are 32
black squares on a board, and 4 of them are occupied by hounds. Since the
hounds do not differ, the number of their positions is $C_{32}^4 = \frac{32
\cdot 31 \cdot 30 \cdot 29}{4!} = 35960$. A fox can be in any of the remaining
28 black squares, so the total number of states is $35960 \cdot 28 + 1 =
1006881$. Here we added 1 because there is a special state when the fox is not
yet on the board.

We have implemented the table as a two-dimensional array, where the index in
one dimension represents a state and in the other -- the action. The state is
encoded as an integer in range $[0, \frac{32!}{27! \cdot 4!}]$ as shown in the
following pseudocode.

\begin{codebox}
    \Procname{$\proc{StateToInt}(fox, hounds)$}
    \li $NUM\_STATES = 32! / 28! / 4! \cdot 28 + 1$
    \li \If $\proc{NotOnTheBoard}(fox)$
    \li     \Then \Return $NUM\_STATES - 1$
        \End
    \li $state \gets 0$
    \li $foxCoordinate \gets 4 \cdot fox.row + fox.column$
    \li \For $i \gets 1$ \To 4
    \li \Do $houndCoordinate \gets 4 \cdot hounds[i].row + hounds[i].column$
    \li     $state \gets state + C_{houndCoordinate}^i$
    \li     \If $fox.row > hounds[i].row$ \kw{or}
    \li \>\> $fox.row = hounds[i].row$ \kw{and} $fox.column > hounds[i].column$
    \li         \Then $foxCoordinate \gets foxCoordinate - 1$
            \End
        \End
    \li \Return $intState \cdot 28 + foxCoordinate$
    \End
\end{codebox}

The columns here are integers from 0 to 3 and the rows are integers from 0 to 7.
The coordinates are then converted to integers by calculating $4 \cdot row +
column$. Arrangement of the hounds is then one of 4-element subsets of a set of
$8 \cdot 4 = 32$ elements. Lines 6--8 implement a method to enumerate those
combinations. Position of the fox is then represented as a number from 0 to 27,
which is an index of an empty square in a sequence of empty squares ordered by
increasing coordinate after placing the hounds. The special case when the fox
is not on the board is encoded as the number of other possible states.

It can be proven that each state can only occur after a move of a certain
player. Notice that the sum of the rows of all pieces modulo 2 is an invariant
with value 0 before fox's move and 1 before hounds' move. This means that the
number of states for one learning system's table of Q-values can be half of the
total number of states.

\subsection{Actions}
Each fox can do at most 4 actions -- move to any of the unoccupied squares. The
hounds can do at most 8 actions -- each of the 4 hounds can move in up to 2
directions.

\section{Experiments}
To run an experiment we repeat the following two phases a hundred times:
\begin{enumerate}
    \item Training phase: let the training systems play against each other for
        100,000 turns.
    \item Evaluation phase: set learning rates to 0. Set the exploration
        rate of one system to 0, play 100 games and record the number of wins
        of the other system. Then do the same evaluation of the other learning
        system. Reset the learning rates and exploration rates to the initial
        values.
\end{enumerate}

\subsection{Experiment 1}
To begin we present the results of an experiment of both fox and hounds playing
randomly (all parameters are 0). Clearly the fox has an advantage in this
situation.

\input{exp1.tex}

\subsection{Experiment 2}
In this experiment we wanted to find out the best parameters for training the
hounds against randomly playing fox.

\input{exp2graph1.tex}

\input{exp2graph2.tex}

\subsection{Experiment 3}
This time we trained the foxes the same way as in experiment 2 we trained the
hounds.

\input{exp3graph1.tex}

\input{exp3graph2.tex}

\subsection{Experiment 4}
In this experiment we can see the results of further training first two hounds
from experiment 2 against first two foxes from experiment 3.

\input{exp4graph1.tex}

\input{exp4graph2.tex}

\input{exp4graph3.tex}

\input{exp4graph4.tex}

As we can see the hounds trained in experiment 2 loose seldom. Foxes are also
well trained and can often take advantage of even such a well-trained hounds'
exploration -- in the cases where hounds' exploration was higher, foxes
performed better.

\subsection{Experiment 5}
The following graphs show performance of both players learning from scratch.

\input{exp5graph1.tex}

\input{exp5graph2.tex}

\input{exp5graph3.tex}

\input{exp5graph4.tex}

\section{Conclusions and Future Work}

\begin{thebibliography}{9}
\bibitem{watkins89}
    Chris Watkins,
    {\em Learning from Delayed Rewards}.
    PhD Thesis, University of Cambridge, 1989.
\bibitem{gerald95}
    Gerald Tesauro,
    {\em Temporal difference learning and TD-Gammon}.
    Communications of the ACM,
    Volume 38 Issue 3, March 1995.
\bibitem{berlekamp82}
    E. Berlekamp, J. Conway, R. Guy,
    {\em Winning Ways for your Mathematical Plays}.
    Volume 2, 1982, p. 635--645.
\end{thebibliography}

\end{document}

